import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, accuracy_score
import joblib
import re
import warnings
warnings.filterwarnings('ignore')

def rule_based_classifier(text):
    """–ü—Ä–∞–≤–∏–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    text_lower = text.lower()
    
    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    platform_score = 0
    commercial_score = 0
    free_score = 0
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è platform —Å –≤–µ—Å–∞–º–∏
    platform_keywords = {
        'platform': 3, 'architecture': 2, 'framework': 2, 'infrastructure': 2, 'system': 1,
        'modular': 2, 'scalable': 2, 'extensible': 2, 'api': 2, 'sdk': 2,
        'integration': 1, 'ecosystem': 2, 'foundation': 1, 'base': 1, 'core': 1,
        'cartridge': 2, 'modular cartridges': 4, 'platform architecture': 4,
        'engine': 1, 'interface': 1, 'component': 1, 'module': 1, 'service': 1
    }
    platform_score = sum(weight for word, weight in platform_keywords.items() if word in text_lower)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è commercial —Å –≤–µ—Å–∞–º–∏
    commercial_keywords = {
        'licensable': 4, 'license': 3, 'licensing': 3, 'commercial': 2, 'market': 1,
        'revenue': 2, 'profit': 2, 'business': 1, 'enterprise': 1, 'industry': 1,
        'commercialization': 4, 'monetization': 3, 'patentable': 4, 'patent': 3,
        'invention': 2, 'novel': 2, 'unique': 1, 'proprietary': 3, 'exclusive': 3,
        'original': 1, 'innovative': 2, 'breakthrough': 3, 'discovery': 2,
        'method': 1, 'process': 1, 'technique': 1, 'approach': 1, 'solution': 1,
        'valuable': 2, 'profitable': 2, 'marketable': 2, 'economic': 1
    }
    commercial_score = sum(weight for word, weight in commercial_keywords.items() if word in text_lower)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è free —Å –≤–µ—Å–∞–º–∏
    free_keywords = {
        'free': 3, 'open': 2, 'public': 1, 'available': 1, 'accessible': 1,
        'unrestricted': 3, 'unlimited': 3, 'unencumbered': 3, 'clear': 1,
        'unobstructed': 3, 'unimpeded': 3, 'unrestrained': 3, 'unfettered': 3,
        'freetooperate': 4, 'free to operate': 4, 'freedom to operate': 4,
        'unconstrained': 3, 'unlimited use': 3, 'open source': 3, 'public domain': 3
    }
    free_score = sum(weight for word, weight in free_keywords.items() if word in text_lower)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
    scores = {
        'platform': platform_score,
        'commercial': commercial_score,
        'free': free_score
    }
    
    # –ï—Å–ª–∏ –≤—Å–µ —Å–∫–æ—Ä—ã —Ä–∞–≤–Ω—ã 0, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É
    if max(scores.values()) == 0:
        if any(word in text_lower for word in ['technology', 'innovation', 'development', 'research']):
            return 'commercial'
        elif any(word in text_lower for word in ['system', 'method', 'approach']):
            return 'platform'
        else:
            return 'commercial'
    
    return max(scores, key=scores.get)

def extract_enhanced_features(text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π"""
    
    text_lower = text.lower()
    
    features = {
        # –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        'text_length': len(text),
        'word_count': len(text.split()),
        'avg_word_length': np.mean([len(w) for w in text.split()]) if text.split() else 0,
        'unique_word_ratio': len(set(text.split())) / len(text.split()) if text.split() else 0,
        
        # –°–∫–æ—Ä–∏–Ω–≥ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        'platform_score': 0,
        'commercial_score': 0,
        'free_score': 0,
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        'technical_terms': 0,
        'scientific_terms': 0,
        'business_terms': 0,
        'engineering_terms': 0,
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        'has_numbers': int(bool(re.search(r'\d+', text))),
        'has_measurements': int(bool(re.search(r'\d+\s*(mg|ml|kg|g|m|cm|mm|nm|Œºm|%)', text))),
        'has_percentages': int('%' in text or 'percent' in text_lower),
        'has_comparisons': int(any(word in text_lower for word in ['better', 'improved', 'enhanced', 'superior', 'advanced', 'increased', 'reduced'])),
        'has_quantifiers': int(any(word in text_lower for word in ['high', 'low', 'large', 'small', 'big', 'tiny', 'massive', 'minimal'])),
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        'has_platform_patterns': 0,
        'has_commercial_patterns': 0,
        'has_free_patterns': 0,
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        'complexity_score': 0,
        'technical_density': 0,
        
        # –ù–æ–≤—ã–µ —Ñ–∏—á–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        'sentence_count': len([s for s in text.split('.') if s.strip()]),
        'question_marks': text.count('?'),
        'exclamation_marks': text.count('!'),
        'has_capitals': int(any(c.isupper() for c in text)),
        'has_parentheses': int('(' in text and ')' in text),
        'has_quotes': int('"' in text or "'" in text),
        'has_dashes': int('-' in text),
        'has_colons': int(':' in text),
        'has_semicolons': int(';' in text),
    }
    
    # –ü–æ–¥—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –≤–µ—Å–∞–º–∏
    platform_keywords = {
        'platform': 3, 'architecture': 2, 'framework': 2, 'infrastructure': 2, 'system': 1,
        'modular': 2, 'scalable': 2, 'extensible': 2, 'api': 2, 'sdk': 2,
        'integration': 1, 'ecosystem': 2, 'foundation': 1, 'base': 1, 'core': 1,
        'cartridge': 2, 'modular cartridges': 4, 'platform architecture': 4,
        'engine': 1, 'interface': 1, 'component': 1, 'module': 1, 'service': 1
    }
    features['platform_score'] = sum(weight for word, weight in platform_keywords.items() if word in text_lower)
    
    commercial_keywords = {
        'licensable': 4, 'license': 3, 'licensing': 3, 'commercial': 2, 'market': 1,
        'revenue': 2, 'profit': 2, 'business': 1, 'enterprise': 1, 'industry': 1,
        'commercialization': 4, 'monetization': 3, 'patentable': 4, 'patent': 3,
        'invention': 2, 'novel': 2, 'unique': 1, 'proprietary': 3, 'exclusive': 3,
        'original': 1, 'innovative': 2, 'breakthrough': 3, 'discovery': 2,
        'method': 1, 'process': 1, 'technique': 1, 'approach': 1, 'solution': 1,
        'valuable': 2, 'profitable': 2, 'marketable': 2, 'economic': 1
    }
    features['commercial_score'] = sum(weight for word, weight in commercial_keywords.items() if word in text_lower)
    
    free_keywords = {
        'free': 3, 'open': 2, 'public': 1, 'available': 1, 'accessible': 1,
        'unrestricted': 3, 'unlimited': 3, 'unencumbered': 3, 'clear': 1,
        'unobstructed': 3, 'unimpeded': 3, 'unrestrained': 3, 'unfettered': 3,
        'freetooperate': 4, 'free to operate': 4, 'freedom to operate': 4,
        'unconstrained': 3, 'unlimited use': 3, 'open source': 3, 'public domain': 3
    }
    features['free_score'] = sum(weight for word, weight in free_keywords.items() if word in text_lower)
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    technical_words = {
        'algorithm': 2, 'optimization': 2, 'efficiency': 1, 'performance': 1, 'processing': 1,
        'computing': 1, 'analysis': 1, 'synthesis': 1, 'engineering': 1, 'design': 1,
        'implementation': 1, 'development': 1, 'programming': 1, 'coding': 1, 'software': 1,
        'hardware': 1, 'device': 1, 'apparatus': 1, 'machine': 1, 'tool': 1
    }
    features['technical_terms'] = sum(weight for word, weight in technical_words.items() if word in text_lower)
    
    # –ù–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    scientific_words = {
        'research': 1, 'study': 1, 'experiment': 1, 'trial': 1, 'test': 1, 'validation': 1,
        'verification': 1, 'hypothesis': 1, 'theory': 1, 'principle': 1, 'concept': 1,
        'discovery': 1, 'finding': 1, 'result': 1, 'conclusion': 1, 'evidence': 1,
        'data': 1, 'analysis': 1, 'statistics': 1, 'measurement': 1, 'observation': 1
    }
    features['scientific_terms'] = sum(weight for word, weight in scientific_words.items() if word in text_lower)
    
    # –ë–∏–∑–Ω–µ—Å —Ç–µ—Ä–º–∏–Ω—ã
    business_words = {
        'market': 1, 'customer': 1, 'user': 1, 'client': 1, 'product': 1, 'service': 1,
        'sales': 1, 'marketing': 1, 'advertising': 1, 'promotion': 1, 'brand': 1,
        'competition': 1, 'competitive': 1, 'advantage': 1, 'value': 1, 'benefit': 1,
        'cost': 1, 'price': 1, 'investment': 1, 'return': 1, 'roi': 1
    }
    features['business_terms'] = sum(weight for word, weight in business_words.items() if word in text_lower)
    
    # –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    engineering_words = {
        'engineering': 1, 'design': 1, 'construction': 1, 'manufacturing': 1, 'production': 1,
        'assembly': 1, 'fabrication': 1, 'building': 1, 'creating': 1, 'making': 1,
        'developing': 1, 'constructing': 1, 'building': 1, 'creating': 1
    }
    features['engineering_terms'] = sum(weight for word, weight in engineering_words.items() if word in text_lower)
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    features['has_platform_patterns'] = int(any(phrase in text_lower for phrase in [
        'platform architecture', 'modular cartridges', 'scalable system', 'extensible framework',
        'api platform', 'sdk framework', 'integration platform'
    ]))
    
    features['has_commercial_patterns'] = int(any(phrase in text_lower for phrase in [
        'commercial value', 'market opportunity', 'revenue potential', 'business model',
        'licensing opportunity', 'patent application', 'commercialization strategy'
    ]))
    
    features['has_free_patterns'] = int(any(phrase in text_lower for phrase in [
        'free to operate', 'freedom to operate', 'unrestricted use', 'open source',
        'public domain', 'unlimited access', 'clear path'
    ]))
    
    # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
    features['complexity_score'] = len([w for w in text.split() if len(w) > 8])
    features['technical_density'] = features['technical_terms'] / max(features['word_count'], 1)
    
    return features

def extract_features_from_item(item):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏—á–µ–π –∏–∑ item"""
    
    # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
    features = {
        'evidence_count': len(item['evidence']),
        'is_negative': int(item['is_negative']),
        
        # Market features
        'max_importance_score': 0.0,
        'avg_importance_score': 0.0,
        'market_evidence_count': 0,
        'has_product_launch': 0,
        'has_ma': 0,
        'has_partnership': 0,
        'has_funding': 0,
        
        # Trial features
        'max_maturity_score': 0.0,
        'avg_maturity_score': 0.0,
        'trial_evidence_count': 0,
        'has_completed_trial': 0,
        'has_phase3_trial': 0,
        'has_terminated_trial': 0,
        
        # Other evidence counts
        'patent_count': 0,
        'paper_count': 0,
        'disclosure_count': 0,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
    enhanced_features = extract_enhanced_features(item['text'])
    features.update(enhanced_features)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º evidence
    market_scores = []
    trial_scores = []
    
    for ev in item['evidence']:
        if ev['type'] == 'market' and 'importance_score' in ev['meta']:
            features['market_evidence_count'] += 1
            market_scores.append(ev['meta']['importance_score'])
            
            # –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π
            event_type = ev['meta']['type']
            if event_type == 'Product launch':
                features['has_product_launch'] = 1
            elif event_type == 'M&A':
                features['has_ma'] = 1
            elif event_type == 'Partnership':
                features['has_partnership'] = 1
            elif event_type == 'Funding':
                features['has_funding'] = 1
                
        elif ev['type'] == 'trial' and 'maturity_score' in ev['meta']:
            features['trial_evidence_count'] += 1
            trial_scores.append(ev['meta']['maturity_score'])
            
            # –°—Ç–∞—Ç—É—Å—ã –∏ —Ñ–∞–∑—ã
            status = ev['meta']['status']
            phase = ev['meta']['phase']
            
            if status == 'Completed':
                features['has_completed_trial'] = 1
            if phase == 'Phase 3':
                features['has_phase3_trial'] = 1
            if status == 'Terminated':
                features['has_terminated_trial'] = 1
                
        elif ev['type'] == 'patent':
            features['patent_count'] += 1
        elif ev['type'] == 'paper':
            features['paper_count'] += 1
        elif ev['type'] == 'disclosure':
            features['disclosure_count'] += 1
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ—Ä—ã
    if market_scores:
        features['max_importance_score'] = max(market_scores)
        features['avg_importance_score'] = np.mean(market_scores)
    if trial_scores:
        features['max_maturity_score'] = max(trial_scores)
        features['avg_maturity_score'] = np.mean(trial_scores)
    
    return features

def prepare_training_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏"""
    
    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = []
    with open('data/train_final.jsonl', 'r') as f:
        for line in f:
            data.append(json.loads(line))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –∏ –ª–µ–π–±–ª—ã
    X = []
    y = []
    texts = []
    
    for record in data:
        for item in record['items']:
            features = extract_features_from_item(item)
            X.append(features)
            
            # –£–ø—Ä–æ—â–∞–µ–º –∫–ª–∞—Å—Å—ã
            original_label = item['label']
            if original_label in ['licensable', 'patentable']:
                simplified_label = 'commercial'
            elif original_label == 'platform':
                simplified_label = 'platform'
            elif original_label == 'freetooperate_flag':
                simplified_label = 'free'
            else:  # none
                simplified_label = 'none'
            
            y.append(simplified_label)
            texts.append(item['text'])
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
    X_df = pd.DataFrame(X)
    y_series = pd.Series(y)
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X_df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {len(X_df.columns)}")
    print(f"‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y_series.value_counts().to_dict()}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º TF-IDF —Ñ–∏—á–∏
    print("üî§ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ TF-IDF —Ñ–∏—á–µ–π...")
    
    tfidf = TfidfVectorizer(
        max_features=500,
        ngram_range=(1, 3),
        stop_words='english',
        min_df=2,
        max_df=0.9
    )
    
    tfidf_features = tfidf.fit_transform(texts)
    tfidf_df = pd.DataFrame(
        tfidf_features.toarray(),
        columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])]
    )
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∏—á–∏
    X_combined = pd.concat([X_df, tfidf_df], axis=1)
    
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {tfidf_features.shape[1]} TF-IDF —Ñ–∏—á–µ–π")
    print(f"‚úÖ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {X_combined.shape[1]}")
    
    return X_combined, y_series, tfidf

def train_final_model(X, y, tfidf):
    """–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Train set: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"Test set: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        min_samples_split=3,
        min_samples_leaf=1,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )
    
    gb = GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=10,
        random_state=42
    )
    
    lr = LogisticRegression(
        random_state=42,
        max_iter=3000,
        C=0.1,
        class_weight='balanced'
    )
    
    # –ê–Ω—Å–∞–º–±–ª—å —Å –≤–µ—Å–∞–º–∏
    ensemble = VotingClassifier([
        ('rf', rf),
        ('gb', gb),
        ('lr', lr)
    ], voting='soft')
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è...")
    ensemble.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = ensemble.predict(X_test)
    y_pred_proba = ensemble.predict_proba(X_test)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"Ensemble Accuracy: {accuracy:.3f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω—Å–∞–º–±–ª—é:")
    print(classification_report(y_test, y_pred))
    
    return ensemble, accuracy

def save_final_model(model, feature_names, tfidf, model_name="final_technology_evaluator"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = f"{model_name}.joblib"
    joblib.dump(model, model_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π
    features_path = f"{model_name}_features.json"
    with open(features_path, 'w') as f:
        json.dump(feature_names.tolist(), f)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º TF-IDF
    tfidf_path = f"{model_name}_tfidf.joblib"
    joblib.dump(tfidf, tfidf_path)
    
    print(f"\nüíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
    print(f"  –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"  –§–∏—á–∏: {features_path}")
    print(f"  TF-IDF: {tfidf_path}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, tfidf = prepare_training_data()
    
    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    ensemble_model, accuracy = train_final_model(X, y, tfidf)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_final_model(ensemble_model, X.columns, tfidf)
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")

if __name__ == "__main__":
    main()
