import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import joblib
import re
import warnings
warnings.filterwarnings('ignore')

def extract_keyword_features(text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    
    text_lower = text.lower()
    
    features = {
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        'licensable_keywords': 0,
        'platform_keywords': 0,
        'patentable_keywords': 0,
        'freetooperate_keywords': 0,
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        'technical_terms': 0,
        'commercial_terms': 0,
        'scientific_terms': 0,
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        'has_numbers': 0,
        'has_measurements': 0,
        'has_percentages': 0,
        'has_comparisons': 0,
        
        # –î–ª–∏–Ω–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        'text_length': len(text),
        'word_count': len(text.split()),
        'avg_word_length': np.mean([len(w) for w in text.split()]) if text.split() else 0,
    }
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è licensable
    licensable_words = [
        'licensable', 'license', 'licensing', 'commercial', 'market', 'revenue',
        'profit', 'business', 'enterprise', 'industry', 'commercialization',
        'monetization', 'royalty', 'franchise', 'distribution'
    ]
    features['licensable_keywords'] = sum(1 for word in licensable_words if word in text_lower)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è platform
    platform_words = [
        'platform', 'architecture', 'framework', 'infrastructure', 'system',
        'modular', 'scalable', 'extensible', 'api', 'sdk', 'integration',
        'ecosystem', 'foundation', 'base', 'core', 'engine'
    ]
    features['platform_keywords'] = sum(1 for word in platform_words if word in text_lower)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è patentable
    patentable_words = [
        'patentable', 'patent', 'invention', 'novel', 'unique', 'proprietary',
        'exclusive', 'original', 'innovative', 'breakthrough', 'discovery',
        'method', 'process', 'technique', 'approach', 'solution'
    ]
    features['patentable_keywords'] = sum(1 for word in patentable_words if word in text_lower)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è freetooperate
    freetooperate_words = [
        'free', 'open', 'public', 'available', 'accessible', 'unrestricted',
        'unlimited', 'unencumbered', 'clear', 'unobstructed', 'unimpeded',
        'unrestrained', 'unfettered', 'unconstrained'
    ]
    features['freetooperate_keywords'] = sum(1 for word in freetooperate_words if word in text_lower)
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    technical_words = [
        'algorithm', 'optimization', 'efficiency', 'performance', 'processing',
        'computing', 'analysis', 'synthesis', 'engineering', 'design',
        'implementation', 'development', 'programming', 'coding', 'software'
    ]
    features['technical_terms'] = sum(1 for word in technical_words if word in text_lower)
    
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    commercial_words = [
        'market', 'customer', 'user', 'client', 'product', 'service',
        'sales', 'marketing', 'advertising', 'promotion', 'brand',
        'competition', 'competitive', 'advantage', 'value', 'benefit'
    ]
    features['commercial_terms'] = sum(1 for word in commercial_words if word in text_lower)
    
    # –ù–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    scientific_words = [
        'research', 'study', 'experiment', 'trial', 'test', 'validation',
        'verification', 'hypothesis', 'theory', 'principle', 'concept',
        'discovery', 'finding', 'result', 'conclusion', 'evidence'
    ]
    features['scientific_terms'] = sum(1 for word in scientific_words if word in text_lower)
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
    features['has_numbers'] = int(bool(re.search(r'\d+', text)))
    features['has_measurements'] = int(bool(re.search(r'\d+\s*(mg|ml|kg|g|m|cm|mm|nm|Œºm|%)', text)))
    features['has_percentages'] = int('%' in text or 'percent' in text_lower)
    features['has_comparisons'] = int(any(word in text_lower for word in ['better', 'improved', 'enhanced', 'superior', 'advanced', 'increased', 'reduced']))
    
    return features

def extract_features_from_item(item):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∏—á–µ–π –∏–∑ item"""
    
    # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
    features = {
        'evidence_count': len(item['evidence']),
        'is_negative': int(item['is_negative']),
        
        # Market features
        'max_importance_score': 0.0,
        'avg_importance_score': 0.0,
        'market_evidence_count': 0,
        'has_product_launch': 0,
        'has_ma': 0,
        'has_partnership': 0,
        'has_funding': 0,
        
        # Trial features
        'max_maturity_score': 0.0,
        'avg_maturity_score': 0.0,
        'trial_evidence_count': 0,
        'has_completed_trial': 0,
        'has_phase3_trial': 0,
        'has_terminated_trial': 0,
        
        # Other evidence counts
        'patent_count': 0,
        'paper_count': 0,
        'disclosure_count': 0,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    keyword_features = extract_keyword_features(item['text'])
    features.update(keyword_features)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º evidence
    market_scores = []
    trial_scores = []
    
    for ev in item['evidence']:
        if ev['type'] == 'market' and 'importance_score' in ev['meta']:
            features['market_evidence_count'] += 1
            market_scores.append(ev['meta']['importance_score'])
            
            # –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π
            event_type = ev['meta']['type']
            if event_type == 'Product launch':
                features['has_product_launch'] = 1
            elif event_type == 'M&A':
                features['has_ma'] = 1
            elif event_type == 'Partnership':
                features['has_partnership'] = 1
            elif event_type == 'Funding':
                features['has_funding'] = 1
                
        elif ev['type'] == 'trial' and 'maturity_score' in ev['meta']:
            features['trial_evidence_count'] += 1
            trial_scores.append(ev['meta']['maturity_score'])
            
            # –°—Ç–∞—Ç—É—Å—ã –∏ —Ñ–∞–∑—ã
            status = ev['meta']['status']
            phase = ev['meta']['phase']
            
            if status == 'Completed':
                features['has_completed_trial'] = 1
            if phase == 'Phase 3':
                features['has_phase3_trial'] = 1
            if status == 'Terminated':
                features['has_terminated_trial'] = 1
                
        elif ev['type'] == 'patent':
            features['patent_count'] += 1
        elif ev['type'] == 'paper':
            features['paper_count'] += 1
        elif ev['type'] == 'disclosure':
            features['disclosure_count'] += 1
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ—Ä—ã
    if market_scores:
        features['max_importance_score'] = max(market_scores)
        features['avg_importance_score'] = np.mean(market_scores)
    if trial_scores:
        features['max_maturity_score'] = max(trial_scores)
        features['avg_maturity_score'] = np.mean(trial_scores)
    
    return features

def prepare_training_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = []
    with open('data/train_final.jsonl', 'r') as f:
        for line in f:
            data.append(json.loads(line))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –∏ –ª–µ–π–±–ª—ã
    X = []
    y = []
    texts = []
    
    for record in data:
        for item in record['items']:
            features = extract_features_from_item(item)
            X.append(features)
            y.append(item['label'])
            texts.append(item['text'])
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
    X_df = pd.DataFrame(X)
    y_series = pd.Series(y)
    
    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X_df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {len(X_df.columns)}")
    print(f"‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y_series.value_counts().to_dict()}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º TF-IDF —Ñ–∏—á–∏
    print("üî§ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ TF-IDF —Ñ–∏—á–µ–π...")
    
    tfidf = TfidfVectorizer(
        max_features=500,
        ngram_range=(1, 3),
        stop_words='english',
        min_df=3,
        max_df=0.9
    )
    
    tfidf_features = tfidf.fit_transform(texts)
    tfidf_df = pd.DataFrame(
        tfidf_features.toarray(),
        columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])]
    )
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∏—á–∏
    X_combined = pd.concat([X_df, tfidf_df], axis=1)
    
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {tfidf_features.shape[1]} TF-IDF —Ñ–∏—á–µ–π")
    print(f"‚úÖ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {X_combined.shape[1]}")
    
    return X_combined, y_series, tfidf

def train_models(X, y):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤"""
    
    print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤...")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weight_dict = dict(zip(np.unique(y), class_weights))
    
    print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weight_dict}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Train set: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"Test set: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ú–æ–¥–µ–ª–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤
    models = {
        'Random Forest': RandomForestClassifier(
            n_estimators=300,
            max_depth=25,
            min_samples_split=3,
            min_samples_leaf=1,
            class_weight='balanced',
            random_state=42
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=15,
            random_state=42
        ),
        'Logistic Regression': LogisticRegression(
            random_state=42,
            max_iter=3000,
            C=0.1,
            class_weight='balanced'
        )
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n–û–±—É—á–µ–Ω–∏–µ {name}...")
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'predictions': y_pred,
            'y_test': y_test
        }
        
        print(f"Accuracy: {accuracy:.3f}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    best_model_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
    best_model = results[best_model_name]['model']
    
    print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"Accuracy: {results[best_model_name]['accuracy']:.3f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ {best_model_name}:")
    print(classification_report(
        results[best_model_name]['y_test'], 
        results[best_model_name]['predictions']
    ))
    
    # –í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nüîç –¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á–µ–π:")
        for _, row in feature_importance.head(20).iterrows():
            print(f"  {row['feature']}: {row['importance']:.3f}")
    
    return best_model, X.columns, results

def save_model(model, feature_names, tfidf, model_name="technology_evaluator_keywords"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = f"{model_name}.joblib"
    joblib.dump(model, model_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π
    features_path = f"{model_name}_features.json"
    with open(features_path, 'w') as f:
        json.dump(feature_names.tolist(), f)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º TF-IDF
    tfidf_path = f"{model_name}_tfidf.joblib"
    joblib.dump(tfidf, tfidf_path)
    
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
    print(f"  –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"  –§–∏—á–∏: {features_path}")
    print(f"  TF-IDF: {tfidf_path}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –ö–õ–Æ–ß–ï–í–´–ú–ò –°–õ–û–í–ê–ú–ò")
    print("=" * 60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, tfidf = prepare_training_data()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    best_model, feature_names, results = train_models(X, y)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_model(best_model, feature_names, tfidf)
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == "__main__":
    main()
