import json
import pandas as pd
import numpy as np
import re
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib
import warnings
warnings.filterwarnings('ignore')

def extract_patent_features(text):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π –¥–ª—è –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    text_lower = text.lower()
    
    features = {
        # –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        'text_length': len(text),
        'word_count': len(text.split()),
        'avg_word_length': np.mean([len(w) for w in text.split()]) if text.split() else 0,
        'unique_word_ratio': len(set(text.split())) / len(text.split()) if text.split() else 0,
        'sentence_count': len([s for s in text.split('.') if s.strip()]),
        
        # –ü–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –≤—ã—Å–æ–∫–∏–º–∏ –≤–µ—Å–∞–º–∏
        'patent_keyword_score': 0,
        'novelty_score': 0,
        'innovation_score': 0,
        'technical_complexity_score': 0,
        'commercial_potential_score': 0,
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        'has_patent_terms': 0,
        'has_invention_terms': 0,
        'has_novelty_terms': 0,
        'has_technical_terms': 0,
        'has_commercial_terms': 0,
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
        'complexity_score': 0,
        'technical_density': 0,
        'has_measurements': 0,
        'has_numbers': 0,
        'has_specifics': 0,
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        'has_method_description': 0,
        'has_process_description': 0,
        'has_device_description': 0,
        'has_composition_description': 0,
    }
    
    # –ü–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –≤–µ—Å–∞–º–∏
    patent_keywords = {
        'patent': 8, 'patentable': 8, 'intellectual property': 8, 'ip': 6,
        'invention': 7, 'novel': 7, 'new': 5, 'original': 5,
        'innovative': 6, 'breakthrough': 7, 'revolutionary': 7,
        'unique': 5, 'proprietary': 6, 'exclusive': 6,
        'method': 4, 'process': 4, 'technique': 4, 'approach': 3,
        'device': 4, 'apparatus': 4, 'system': 3, 'mechanism': 4,
        'composition': 5, 'formula': 5, 'compound': 5, 'material': 3,
        'algorithm': 5, 'software': 3, 'application': 3,
        'discovery': 6, 'development': 3, 'research': 2,
        'manufacturing': 4, 'production': 4, 'synthesis': 4,
        'enabling': 4, 'scalable': 4, 'efficient': 3,
        'optimized': 3, 'improved': 4, 'enhanced': 4,
        'advanced': 4, 'cutting-edge': 6, 'state-of-the-art': 6
    }
    
    features['patent_keyword_score'] = sum(weight for word, weight in patent_keywords.items() if word in text_lower)
    
    # –¢–µ—Ä–º–∏–Ω—ã –Ω–æ–≤–∏–∑–Ω—ã
    novelty_terms = {
        'novel': 6, 'new': 4, 'original': 4, 'first': 5,
        'unprecedented': 6, 'never before': 6, 'groundbreaking': 6,
        'pioneering': 5, 'innovative': 5, 'revolutionary': 6
    }
    features['novelty_score'] = sum(weight for word, weight in novelty_terms.items() if word in text_lower)
    
    # –¢–µ—Ä–º–∏–Ω—ã –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
    innovation_terms = {
        'innovation': 5, 'breakthrough': 6, 'advance': 4,
        'improvement': 4, 'enhancement': 4, 'optimization': 4,
        'discovery': 5, 'invention': 6, 'creation': 4,
        'development': 3, 'research': 2, 'study': 2
    }
    features['innovation_score'] = sum(weight for word, weight in innovation_terms.items() if word in text_lower)
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    technical_terms = {
        'complex': 4, 'sophisticated': 4, 'advanced': 4,
        'cutting-edge': 6, 'state-of-the-art': 6, 'high-performance': 4,
        'optimized': 3, 'efficient': 3, 'robust': 3, 'reliable': 3,
        'algorithm': 5, 'method': 4, 'process': 4, 'technique': 4,
        'engineering': 4, 'design': 3, 'implementation': 3,
        'manufacturing': 4, 'production': 4, 'fabrication': 4,
        'synthesis': 4, 'analysis': 3, 'processing': 3
    }
    features['technical_complexity_score'] = sum(weight for word, weight in technical_terms.items() if word in text_lower)
    
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
    commercial_terms = {
        'commercial': 5, 'market': 4, 'business': 3, 'revenue': 5,
        'profit': 5, 'valuable': 4, 'profitable': 5, 'marketable': 5,
        'licensable': 6, 'license': 5, 'licensing': 5,
        'enterprise': 4, 'industry': 3, 'economic': 3,
        'commercialization': 6, 'monetization': 5
    }
    features['commercial_potential_score'] = sum(weight for word, weight in commercial_terms.items() if word in text_lower)
    
    # –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
    features['has_patent_terms'] = int(any(term in text_lower for term in ['patent', 'patentable', 'intellectual property', 'ip']))
    features['has_invention_terms'] = int(any(term in text_lower for term in ['invention', 'invent', 'inventor']))
    features['has_novelty_terms'] = int(any(term in text_lower for term in ['novel', 'new', 'original', 'first', 'unprecedented']))
    features['has_technical_terms'] = int(any(term in text_lower for term in ['technical', 'technology', 'engineering', 'algorithm', 'method']))
    features['has_commercial_terms'] = int(any(term in text_lower for term in ['commercial', 'market', 'business', 'revenue', 'profit']))
    
    # –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
    features['complexity_score'] = len([w for w in text.split() if len(w) > 8])
    features['technical_density'] = features['technical_complexity_score'] / max(features['word_count'], 1)
    features['has_measurements'] = int(bool(re.search(r'\d+\s*(mg|ml|kg|g|m|cm|mm|nm|Œºm|%|hz|mhz|ghz)', text_lower)))
    features['has_numbers'] = int(bool(re.search(r'\d+', text)))
    features['has_specifics'] = int(any(word in text_lower for word in ['specific', 'precise', 'exact', 'detailed', 'comprehensive']))
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
    features['has_method_description'] = int(any(phrase in text_lower for phrase in ['method for', 'method of', 'method to', 'process for', 'process of']))
    features['has_process_description'] = int(any(phrase in text_lower for phrase in ['process for', 'process of', 'process to', 'procedure for', 'procedure of']))
    features['has_device_description'] = int(any(phrase in text_lower for phrase in ['device for', 'device of', 'apparatus for', 'system for', 'machine for']))
    features['has_composition_description'] = int(any(phrase in text_lower for phrase in ['composition of', 'formula for', 'compound of', 'mixture of', 'blend of']))
    
    return features

def prepare_patent_training_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = []
    with open('data/train_final.jsonl', 'r') as f:
        for line in f:
            item = json.loads(line)
            data.append(item)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    X_texts = []
    y_patent = []
    
    for item in data:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º items –∏–∑ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏
        for sub_item in item['items']:
            text = sub_item['text']
            original_label = sub_item['label']
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ª–µ–π–±–ª–æ–≤
            if original_label in ['patentable', 'licensable']:
                patent_label = 1  # –í—ã—Å–æ–∫–∏–π –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
            elif original_label == 'platform':
                patent_label = 0  # –ù–∏–∑–∫–∏–π –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
            elif original_label == 'freetooperate_flag':
                patent_label = 0  # –ù–∏–∑–∫–∏–π –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
            else:  # none
                patent_label = 0  # –ù–∏–∑–∫–∏–π –ø–∞—Ç–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
            
            X_texts.append(text)
            y_patent.append(patent_label)
    
    print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫:")
    print(f"  –í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (1): {sum(y_patent)}")
    print(f"  –ù–∏–∑–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (0): {len(y_patent) - sum(y_patent)}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏
    print("üîß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏—á–µ–π...")
    X_features = []
    for text in X_texts:
        features = extract_patent_features(text)
        X_features.append(features)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    X_df = pd.DataFrame(X_features)
    
    # –î–æ–±–∞–≤–ª—è–µ–º TF-IDF —Ñ–∏—á–∏
    print("üìù –°–æ–∑–¥–∞–Ω–∏–µ TF-IDF —Ñ–∏—á–µ–π...")
    tfidf = TfidfVectorizer(
        max_features=300,  # –ú–µ–Ω—å—à–µ —Ñ–∏—á–µ–π –¥–ª—è –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        ngram_range=(1, 2),  # –¢–æ–ª—å–∫–æ –±–∏–≥—Ä–∞–º–º—ã
        stop_words='english',
        min_df=3,
        max_df=0.8
    )
    
    tfidf_features = tfidf.fit_transform(X_texts)
    tfidf_df = pd.DataFrame(
        tfidf_features.toarray(),
        columns=[f'patent_tfidf_{i}' for i in range(tfidf_features.shape[1])]
    )
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∏—á–∏
    X_combined = pd.concat([X_df, tfidf_df], axis=1)
    
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ —Ñ–∏—á–∏: {X_combined.shape[1]}")
    print(f"  –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏: {X_df.shape[1]}")
    print(f"  TF-IDF —Ñ–∏—á–∏: {tfidf_df.shape[1]}")
    
    return X_combined, np.array(y_patent), tfidf, X_df.columns.tolist()

def train_patent_model():
    """–û–±—É—á–µ–Ω–∏–µ –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("üöÄ –û–±—É—á–µ–Ω–∏–µ –ø–∞—Ç–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y, tfidf, feature_names = prepare_patent_training_data()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        class_weight='balanced'
    )
    
    gb = GradientBoostingClassifier(
        n_estimators=150,
        learning_rate=0.1,
        max_depth=8,
        random_state=42
    )
    
    lr = LogisticRegression(
        random_state=42,
        class_weight='balanced',
        max_iter=1000
    )
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
    patent_model = VotingClassifier(
        estimators=[
            ('rf', rf),
            ('gb', gb),
            ('lr', lr)
        ],
        voting='soft'
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    patent_model.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    y_pred = patent_model.predict(X_test)
    y_pred_proba = patent_model.predict_proba(X_test)[:, 1]
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_score(y_test, y_pred):.4f}")
    print("\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    print(classification_report(y_test, y_pred, target_names=['Low Patent Potential', 'High Patent Potential']))
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    print("\nüîÑ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (5-fold):")
    cv_scores = cross_val_score(patent_model, X, y, cv=5, scoring='accuracy')
    print(f"–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á–µ–π)
    print("\nüîç –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á–µ–π:")
    base_feature_importance = patent_model.named_estimators_['rf'].feature_importances_[:len(feature_names)]
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': base_feature_importance
    }).sort_values('importance', ascending=False)
    
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
        print(f"  {i+1}. {row['feature']}: {row['importance']:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ñ–∏—á–∏
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    joblib.dump(patent_model, 'patent_potential_model.joblib')
    joblib.dump(tfidf, 'patent_potential_tfidf.joblib')
    
    with open('patent_potential_features.json', 'w') as f:
        json.dump(feature_names, f)
    
    print("‚úÖ –ü–∞—Ç–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    print("   - patent_potential_model.joblib")
    print("   - patent_potential_tfidf.joblib")
    print("   - patent_potential_features.json")
    
    return patent_model, tfidf, feature_names

if __name__ == "__main__":
    train_patent_model()
